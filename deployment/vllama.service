[Unit]
Description=vllama - High-performance LLM inference server
Documentation=https://github.com/nijaru/vllama
After=network-online.target
Wants=network-online.target

[Service]
Type=simple
User=vllama
Group=vllama
WorkingDirectory=/opt/vllama

# Environment
Environment="RUST_LOG=info"
Environment="VLLAMA_LOG_FORMAT=json"
Environment="PATH=/usr/local/bin:/usr/bin:/bin"

# Start command
ExecStart=/usr/local/bin/vllama serve \
    --host 127.0.0.1 \
    --port 11434 \
    --model Qwen/Qwen2.5-7B-Instruct \
    --gpu-memory-utilization 0.9 \
    --max-num-seqs 256

# Logging
StandardOutput=journal
StandardError=journal
SyslogIdentifier=vllama

# Restart policy
Restart=on-failure
RestartSec=10s

# Security hardening
NoNewPrivileges=true
PrivateTmp=true
ProtectSystem=strict
ProtectHome=true
ReadWritePaths=/var/log/vllama /home/vllama/.cache/huggingface

# Resource limits
LimitNOFILE=65536
LimitNPROC=4096

# Process management
KillMode=mixed
KillSignal=SIGTERM
TimeoutStopSec=30

[Install]
WantedBy=multi-user.target
