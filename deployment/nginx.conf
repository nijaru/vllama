# vllama reverse proxy configuration for nginx
# Place in /etc/nginx/sites-available/vllama
# Symlink to /etc/nginx/sites-enabled/vllama

upstream vllama_backend {
    server 127.0.0.1:11434;
    keepalive 32;
}

# Rate limiting zones
limit_req_zone $binary_remote_addr zone=vllama_limit:10m rate=10r/s;
limit_req_status 429;

# HTTP server (redirect to HTTPS)
server {
    listen 80;
    listen [::]:80;
    server_name llm.example.com;

    location / {
        return 301 https://$server_name$request_uri;
    }
}

# HTTPS server
server {
    listen 443 ssl http2;
    listen [::]:443 ssl http2;
    server_name llm.example.com;

    # SSL configuration
    ssl_certificate /etc/letsencrypt/live/llm.example.com/fullchain.pem;
    ssl_certificate_key /etc/letsencrypt/live/llm.example.com/privkey.pem;
    ssl_protocols TLSv1.2 TLSv1.3;
    ssl_ciphers HIGH:!aNULL:!MD5;
    ssl_prefer_server_ciphers on;
    ssl_session_cache shared:SSL:10m;
    ssl_session_timeout 10m;

    # Security headers
    add_header Strict-Transport-Security "max-age=31536000; includeSubDomains" always;
    add_header X-Content-Type-Options "nosniff" always;
    add_header X-Frame-Options "DENY" always;
    add_header X-XSS-Protection "1; mode=block" always;

    # Logging
    access_log /var/log/nginx/vllama_access.log combined;
    error_log /var/log/nginx/vllama_error.log warn;

    # Client settings
    client_max_body_size 10M;
    client_body_timeout 60s;
    client_header_timeout 60s;

    # Proxy settings
    proxy_http_version 1.1;
    proxy_set_header Host $host;
    proxy_set_header X-Real-IP $remote_addr;
    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
    proxy_set_header X-Forwarded-Proto $scheme;
    proxy_set_header Connection "";

    # Timeouts for LLM inference (can be slow)
    proxy_connect_timeout 60s;
    proxy_send_timeout 300s;
    proxy_read_timeout 300s;

    # Buffering (disabled for streaming)
    proxy_buffering off;
    proxy_request_buffering off;

    # Main API endpoints
    location /api/ {
        # Rate limiting
        limit_req zone=vllama_limit burst=20 nodelay;

        # Proxy to vllama
        proxy_pass http://vllama_backend;
    }

    # Health check endpoint (no rate limiting)
    location /health {
        proxy_pass http://vllama_backend;
        access_log off;
    }

    # OpenAI-compatible endpoint
    location /v1/ {
        # Rate limiting
        limit_req zone=vllama_limit burst=20 nodelay;

        # Proxy to vllama
        proxy_pass http://vllama_backend;
    }

    # Optional: Basic authentication
    # Uncomment to require authentication
    # auth_basic "vllama API";
    # auth_basic_user_file /etc/nginx/.htpasswd;
}
