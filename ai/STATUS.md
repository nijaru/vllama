# Status

_Last Updated: 2025-10-29_

## Current State

**Version:** 0.0.5 (in progress)
**Focus:** Linux + NVIDIA production deployments

**Strategy:** "Ollama's DX with vLLM's performance"
- Target: Production Linux with NVIDIA GPUs
- NOT targeting: macOS/hobbyists (Ollama great there)
- NOT targeting: Researchers (use raw vLLM)

**Performance (RTX 4090, 30% GPU utilization):**
- Sequential: 232ms (4.4x faster than Ollama - Qwen 1.5B) ‚úÖ
- Concurrent (5): 0.217s (29.95x faster than Ollama 6.50s - facebook/opt-125m) ‚úÖ‚úÖ‚úÖ
- Concurrent (50): 2.115s (maintains 23.6 req/s throughput - facebook/opt-125m) ‚úÖ
- Streaming: 0.617s (1.6x faster than Ollama - Qwen 1.5B) ‚úÖ

**Endpoints:**
- ‚úÖ /api/generate (streaming + non-streaming)
- ‚úÖ /api/chat (proper chat templates via vLLM)
- ‚úÖ /api/pull (HuggingFace model downloads)
- ‚úÖ /api/tags (list models)
- ‚úÖ /api/ps (queries vLLM for running models)
- ‚úÖ /api/show (queries vLLM for model metadata)
- ‚úÖ /api/version (returns vllama version)
- ‚ùå /api/embeddings (skipped for 0.0.x - RAG use case)

**Platform support:**
- ‚úÖ Linux + NVIDIA GPU (production ready)
- ‚ö†Ô∏è macOS CPU-only (experimental, slow - need llama.cpp)

## What Worked

**Phase 4+ cleanup:**
- Removed all custom wrappers (1,165 lines deleted)
- Uses vLLM official OpenAI server directly
- Clean architecture: Client ‚Üí vllama (Rust) ‚Üí vLLM OpenAI Server ‚Üí GPU
- uv integration for Python environment management
- Proper chat completion endpoint (uses vLLM's /v1/chat/completions)

**Performance optimizations (VERIFIED ‚úÖ):**
- Added --max-num-batched-tokens 16384 (32x increase from default 512)
- Added --enable-chunked-prefill for concurrent batching
- Added --enable-prefix-caching for KV cache reuse
- Removed hardcoded --max-model-len (let vLLM auto-detect)
- **Impact: 34.91x faster than before, 29.95x faster than Ollama!**

## What Didn't Work

**~~Concurrent requests slower than Ollama~~ (FIXED ‚úÖ):**
- Root cause: Using minimal vLLM configuration (only 2 params)
- Missing critical optimization flags
- Fix: Added optimization flags, tested, verified 29.95x faster than Ollama!

**macOS performance:**
- vLLM CPU-only, no Metal support planned
- Need llama.cpp for Apple Silicon (Phase 2)

## Active Work

**0.0.4 Completed:**
- ‚úÖ Tested popular models (Qwen 2.5: 0.5B, 1.5B, 7B; Mistral 7B)
- ‚úÖ Created comprehensive docs/MODELS.md
- ‚úÖ Updated README.md with model references
- ‚úÖ Documented GPU memory requirements (7B needs 90% utilization)
- ‚úÖ Documented authentication requirements for Llama models

**0.0.5 In Progress (Production Polish):**
- ‚úÖ Modern CLI UX with clean symbols (‚Üí ‚Ä¢ ‚úì ‚úó), no emojis
- ‚úÖ Progress indicators (spinner for vLLM startup)
- ‚úÖ Output modes: --quiet, --json for scripting
- ‚úÖ vLLM output redirected to log file (clean terminal)
- ‚úÖ Consistent branding (vllama lowercase everywhere)
- üéØ **Current:** Error handling improvements
- Pending: Enhanced /health endpoint with GPU info
- Pending: Structured logging (JSON format)

**Competitive Analysis Complete:**
- Positioning: "Ollama's DX with vLLM's performance"
- Moat: 20-30x faster concurrent (PagedAttention), production focus
- Target: Linux production deployments, high-throughput APIs
- NOT competing: Cross-platform, GUI, beginner ease

## Blockers

None
