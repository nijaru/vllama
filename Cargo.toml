[workspace]
resolver = "2"
members = [
    "crates/vllama-cli",
    "crates/vllama-server",
    "crates/vllama-core",
    "crates/vllama-engine",
    "crates/vllama-models",
]

[workspace.package]
version = "0.1.0"
edition = "2021"
rust-version = "1.75"
authors = ["vLLama Contributors"]
license = "MIT OR Apache-2.0"
repository = "https://github.com/nijaru/vllama"

[workspace.dependencies]
# Web framework
axum = "0.7"
tokio = { version = "1.35", features = ["full"] }
tower = "0.4"
tower-http = { version = "0.5", features = ["trace", "cors"] }

# CLI
clap = { version = "4.4", features = ["derive", "env"] }

# Serialization
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
toml = "0.8"

# Database
sqlx = { version = "0.7", features = ["runtime-tokio", "sqlite"] }

# HTTP client
reqwest = { version = "0.11", features = ["stream", "json"] }

# HuggingFace Hub
hf-hub = { version = "0.4", features = ["tokio"] }

# Observability
tracing = "0.1"
tracing-subscriber = { version = "0.3", features = ["env-filter", "json"] }

# Error handling
anyhow = "1.0"
thiserror = "1.0"

# Async utilities
futures = "0.3"
async-stream = "0.3"
async-trait = "0.1"
tokio-stream = "0.1"

# Concurrent data structures
dashmap = "5.5"
parking_lot = "0.12"

# Hardware detection
sysinfo = "0.30"

# Workspace crates
vllama-cli = { path = "crates/vllama-cli" }
vllama-server = { path = "crates/vllama-server" }
vllama-core = { path = "crates/vllama-core" }
vllama-engine = { path = "crates/vllama-engine" }
vllama-models = { path = "crates/vllama-models" }

[profile.release]
opt-level = 3
lto = "thin"
codegen-units = 1
strip = true

[profile.bench]
inherits = "release"
